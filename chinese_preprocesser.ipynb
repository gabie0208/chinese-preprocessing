{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "chinese-preprocesser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIf1H1snJW32",
        "outputId": "89d0bbf0-d772-47a2-c0d3-09d4cec66aef"
      },
      "source": [
        "!pip install ltp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ltp\n",
            "  Downloading ltp-4.1.5.post2-py3-none-any.whl (94 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▌                            | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 20 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 94 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from ltp) (1.9.0+cu102)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from ltp) (21.0)\n",
            "Collecting pygtrie<2.5,>=2.3.0\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Collecting transformers<=4.7.0,>=4.0.0\n",
            "  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->ltp) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->ltp) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (4.62.3)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<=4.7.0,>=4.0.0->ltp) (3.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.7.0,>=4.0.0->ltp) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<=4.7.0,>=4.0.0->ltp) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.7.0,>=4.0.0->ltp) (1.0.1)\n",
            "Building wheels for collected packages: pygtrie\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19062 sha256=02e2fd8f5c80ac57c78c6fd4630abfcdcf7db4da9358fa492a5d7e5ca1d00dff\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "Successfully built pygtrie\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, pygtrie, ltp\n",
            "Successfully installed huggingface-hub-0.0.8 ltp-4.1.5.post2 pygtrie-2.4.2 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gOEokUkJJXR"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "from ltp import LTP\n",
        "from pprint import pprint\n",
        "\n",
        "ltp = LTP()"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x42DL2QkJJXS"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJwQg5u2JJXS",
        "outputId": "8c7c1121-7d2b-40fa-90cd-65ea03fa552b"
      },
      "source": [
        "#path = input(\"Type the absolute path name of the folder the text files are in. \")\n",
        "path_o = \"/content/\" # 폴더는 입력 가능(input)하게 설정\n",
        "path_p = \"/content/sample_data/\"\n",
        "\n",
        "text_file_list = []\n",
        "text_file_list.extend(glob.glob(path_o + \"*.txt\"))\n",
        "text_file_list.extend(glob.glob(path_o + \"*.csv\"))\n",
        "text_file_list.extend(glob.glob(path_o + \"*.xls\"))\n",
        "text_file_list.extend(glob.glob(path_o + \"*.xlsx\"))\n",
        "\n",
        "if path_o + \"result.csv\" in text_file_list:\n",
        "    text_file_list.remove(path_o + \"result.csv\")\n",
        "\n",
        "pprint(text_file_list)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/verb_qilai_literature.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHmzmsCFJJXT",
        "outputId": "1160ae2d-0dac-4bb5-9dcc-ca7fbf788b74"
      },
      "source": [
        "ts = time.time()\n",
        "\n",
        "for file in text_file_list:\n",
        "    print(\"File name: {0}\".format(file))\n",
        "    fr = open(file, \"r\")\n",
        "    fw = open(file + \"_p.csv\", 'w')\n",
        "    \n",
        "    fw.write(\"Preprocessed File\\n\")\n",
        "    \n",
        "    lines = fr.readlines()\n",
        "\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File name: /content/verb_qilai_literature.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Ql7r1UKAL6"
      },
      "source": [
        "no_colon = [] \n",
        "no_qm = []\n",
        "no_src = []\n",
        "no_es = []\n",
        "\n",
        "def preprocess_colon(data):  # 콜론 지우기\n",
        "  for line in data:\n",
        "    if ':' in line:                  # 한국어 콜론\n",
        "      line = re.sub(\"^.*\\:\",\"\", line)\n",
        "    elif '：' in line:               # 중국어 콜론\n",
        "      line = re.sub(\"^.*：\",\"\", line)\n",
        "    else:\n",
        "      line\n",
        "    no_colon.append(line)\n",
        "\n",
        "def preprocess_qm(no_colon): # question mark 지우기\n",
        "  for line in no_colon:\n",
        "    line = re.sub(\"\\'|\\\"\",\"\", line)     # '', \"\" (작은 따옴표, 큰 따옴표 삭제)\n",
        "    line = re.sub(\"\\“|\\ “\",\"\", line)        # 중국어 따옴표는 문자로 등록되어 있지 않아 문자처럼 삭제 \n",
        "    line = re.sub(\"\\”|\\ ”\",\"\", line)\n",
        "    line = re.sub(\"\\‘|\\ ‘\",\"\",line)\n",
        "    line = re.sub(\"\\’|\\ ’\",\"\",line)\n",
        "    no_qm.append(line)\n",
        "\n",
        "def preprocess_src(no_qm):  # 출처 지우기\n",
        "  for line in no_qm:\n",
        "    line = re.sub(pattern=r'\\（[^）]*\\）|\\《[^)]*\\》|\\([^)]*\\)', repl='', string=line) #()형식의 출처 《》 형식의 출처\n",
        "    no_src.append(line)\n",
        "\n",
        "def preprocess_es(no_src): # ellipsis 중략 기호는 , comma로 대체체\n",
        "  for line in no_src:\n",
        "    sent = re.sub('\\……|\\——', ',', line)\n",
        "    no_es.append(sent)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L8aSNwLGkEV",
        "outputId": "8ace91b4-9110-47e1-c98d-9422a2eb280a"
      },
      "source": [
        "preprocess_colon(lines)\n",
        "preprocess_qm(no_colon)\n",
        "preprocess_src(no_qm)\n",
        "preprocess_es(no_src)\n",
        "\n",
        "te = time.time()\n",
        "\n",
        "print(\"Time spent: {0:.4f}\".format(te - ts))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time spent: 4.3433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m1cxELTYq17"
      },
      "source": [
        "DATA_OUT_PATH = '/content/sample_data/'\n",
        "DATA_OUT = os.path.join(DATA_OUT_PATH, \"new_\"+'verb_qilai_literature.txt')   # 새로운 파일 이름: new_입력파일 \n",
        "\n",
        "def dump_txt(out_path):\n",
        "    with open(out_path, 'w') as f:\n",
        "        for line in no_es:\n",
        "            \n",
        "            f.write(line+'\\n')\n",
        "\n",
        "dump_txt(DATA_OUT)"
      ],
      "execution_count": 212,
      "outputs": []
    }
  ]
}